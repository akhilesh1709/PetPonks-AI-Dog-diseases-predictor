{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "base_dir = r'D:\\Petponks\\data'\n",
    "train_dir = r'D:\\Petponks\\data_split\\train'\n",
    "val_dir = r'D:\\Petponks\\data_split\\val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data\n",
    "def split_data(source_dir, train_dir, val_dir, split_ratio=0.2):\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_dir = os.path.join(source_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            images = [img for img in os.listdir(class_dir) if img.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            train_images, val_images = train_test_split(images, test_size=split_ratio, random_state=42)\n",
    "            \n",
    "            # Create directories\n",
    "            os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "            os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "            \n",
    "            # Copy images\n",
    "            for img in train_images:\n",
    "                shutil.copy(os.path.join(class_dir, img), os.path.join(train_dir, class_name, img))\n",
    "            for img in val_images:\n",
    "                shutil.copy(os.path.join(class_dir, img), os.path.join(val_dir, class_name, img))\n",
    "\n",
    "    print(f\"Data split complete. Training images: {sum(len(os.listdir(os.path.join(train_dir, d))) for d in os.listdir(train_dir))}\")\n",
    "    print(f\"Validation images: {sum(len(os.listdir(os.path.join(val_dir, d))) for d in os.listdir(val_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split complete. Training images: 350\n",
      "Validation images: 89\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "split_data(base_dir, train_dir, val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "img_size = 299  # InceptionV3 default input size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Data Generators with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class indices: {'Bacterial_dermatosis': 0, 'Fungal_infections': 1, 'Healthy': 2, 'Hypersensitivity_allergic_dermatosis': 3}\n"
     ]
    }
   ],
   "source": [
    "# Print class indices\n",
    "print(\"Class indices:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained InceptionV3 model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model layers\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 23s 1s/step - loss: 2.0479 - accuracy: 0.3400 - val_loss: 2.0660 - val_accuracy: 0.2809 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 8s 719ms/step - loss: 1.6693 - accuracy: 0.4343 - val_loss: 1.5478 - val_accuracy: 0.4382 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 8s 724ms/step - loss: 1.3793 - accuracy: 0.5371 - val_loss: 1.6649 - val_accuracy: 0.4157 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 8s 701ms/step - loss: 1.3363 - accuracy: 0.5600 - val_loss: 1.7212 - val_accuracy: 0.4157 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 8s 725ms/step - loss: 1.1516 - accuracy: 0.5657 - val_loss: 1.2975 - val_accuracy: 0.4607 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 8s 702ms/step - loss: 1.0275 - accuracy: 0.6429 - val_loss: 1.2886 - val_accuracy: 0.5393 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 8s 684ms/step - loss: 1.1420 - accuracy: 0.6171 - val_loss: 1.3721 - val_accuracy: 0.5056 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 8s 685ms/step - loss: 0.9251 - accuracy: 0.6714 - val_loss: 1.3193 - val_accuracy: 0.5393 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 8s 692ms/step - loss: 0.8332 - accuracy: 0.7000 - val_loss: 1.2516 - val_accuracy: 0.5618 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 8s 767ms/step - loss: 0.7926 - accuracy: 0.7229 - val_loss: 1.0527 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 8s 701ms/step - loss: 0.7472 - accuracy: 0.7057 - val_loss: 1.0425 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 8s 704ms/step - loss: 0.7276 - accuracy: 0.7600 - val_loss: 1.0175 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 8s 674ms/step - loss: 0.7118 - accuracy: 0.7571 - val_loss: 1.0815 - val_accuracy: 0.6180 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 8s 693ms/step - loss: 0.6816 - accuracy: 0.7400 - val_loss: 1.0492 - val_accuracy: 0.6742 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 8s 711ms/step - loss: 0.6383 - accuracy: 0.7457 - val_loss: 0.9626 - val_accuracy: 0.6404 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 8s 719ms/step - loss: 0.5820 - accuracy: 0.7800 - val_loss: 0.9554 - val_accuracy: 0.6292 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 8s 722ms/step - loss: 0.6193 - accuracy: 0.7657 - val_loss: 0.9740 - val_accuracy: 0.6742 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 8s 722ms/step - loss: 0.5627 - accuracy: 0.8000 - val_loss: 0.9844 - val_accuracy: 0.6629 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 8s 708ms/step - loss: 0.6009 - accuracy: 0.7857 - val_loss: 0.9801 - val_accuracy: 0.6517 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 8s 749ms/step - loss: 0.5188 - accuracy: 0.8114 - val_loss: 0.9818 - val_accuracy: 0.6517 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 8s 695ms/step - loss: 0.6346 - accuracy: 0.7800 - val_loss: 1.0099 - val_accuracy: 0.6854 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 8s 745ms/step - loss: 0.4939 - accuracy: 0.8371 - val_loss: 1.0120 - val_accuracy: 0.6854 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 8s 695ms/step - loss: 0.5092 - accuracy: 0.7971 - val_loss: 1.0002 - val_accuracy: 0.6742 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 8s 715ms/step - loss: 0.5163 - accuracy: 0.8114 - val_loss: 0.9810 - val_accuracy: 0.6742 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 8s 705ms/step - loss: 0.5067 - accuracy: 0.8400 - val_loss: 0.9950 - val_accuracy: 0.6854 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 8s 722ms/step - loss: 0.5418 - accuracy: 0.7886 - val_loss: 0.9946 - val_accuracy: 0.6854 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=100,  # Increase epochs, early stopping will prevent overfitting\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 0.9554 - accuracy: 0.6292\n",
      "Validation Accuracy: 62.92%\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "print(\"Evaluating model...\")\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:249]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_95 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,987,300\n",
      "Trainable params: 12,297,860\n",
      "Non-trainable params: 10,689,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 14s 837ms/step - loss: 0.9127 - accuracy: 0.7229 - val_loss: 1.0029 - val_accuracy: 0.6742 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 8s 720ms/step - loss: 0.6731 - accuracy: 0.7800 - val_loss: 1.2064 - val_accuracy: 0.6517 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 8s 701ms/step - loss: 0.5881 - accuracy: 0.7800 - val_loss: 1.1046 - val_accuracy: 0.6404 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 8s 700ms/step - loss: 0.4688 - accuracy: 0.8314 - val_loss: 0.9646 - val_accuracy: 0.6966 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 8s 688ms/step - loss: 0.4974 - accuracy: 0.8371 - val_loss: 0.9961 - val_accuracy: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 8s 706ms/step - loss: 0.3251 - accuracy: 0.8914 - val_loss: 1.0821 - val_accuracy: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 8s 699ms/step - loss: 0.3440 - accuracy: 0.8771 - val_loss: 1.1132 - val_accuracy: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 8s 759ms/step - loss: 0.2453 - accuracy: 0.9171 - val_loss: 0.9117 - val_accuracy: 0.7303 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 9s 775ms/step - loss: 0.1874 - accuracy: 0.9286 - val_loss: 0.9292 - val_accuracy: 0.6966 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 8s 718ms/step - loss: 0.2310 - accuracy: 0.9286 - val_loss: 1.0685 - val_accuracy: 0.6854 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 9s 774ms/step - loss: 0.2833 - accuracy: 0.9029 - val_loss: 0.9647 - val_accuracy: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 8s 757ms/step - loss: 0.1507 - accuracy: 0.9457 - val_loss: 0.9324 - val_accuracy: 0.7416 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 8s 735ms/step - loss: 0.1700 - accuracy: 0.9314 - val_loss: 0.8171 - val_accuracy: 0.7640 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 8s 705ms/step - loss: 0.1542 - accuracy: 0.9457 - val_loss: 0.9244 - val_accuracy: 0.7528 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 8s 696ms/step - loss: 0.1920 - accuracy: 0.9314 - val_loss: 0.9757 - val_accuracy: 0.7191 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 8s 717ms/step - loss: 0.1230 - accuracy: 0.9629 - val_loss: 1.0718 - val_accuracy: 0.7303 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 8s 714ms/step - loss: 0.1194 - accuracy: 0.9543 - val_loss: 1.1042 - val_accuracy: 0.7528 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 8s 718ms/step - loss: 0.1658 - accuracy: 0.9486 - val_loss: 1.1256 - val_accuracy: 0.7640 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 8s 727ms/step - loss: 0.1446 - accuracy: 0.9571 - val_loss: 1.0835 - val_accuracy: 0.7640 - lr: 2.0000e-05\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 8s 718ms/step - loss: 0.1138 - accuracy: 0.9657 - val_loss: 1.0524 - val_accuracy: 0.7640 - lr: 2.0000e-05\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 8s 712ms/step - loss: 0.1177 - accuracy: 0.9543 - val_loss: 1.0182 - val_accuracy: 0.7640 - lr: 2.0000e-05\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 8s 716ms/step - loss: 0.0752 - accuracy: 0.9743 - val_loss: 0.9962 - val_accuracy: 0.7640 - lr: 2.0000e-05\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 8s 720ms/step - loss: 0.0729 - accuracy: 0.9800 - val_loss: 0.9757 - val_accuracy: 0.7640 - lr: 2.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning training\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model...\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.8171 - accuracy: 0.7640\n",
      "Final Validation Accuracy: 76.40%\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_95 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,987,300\n",
      "Trainable params: 12,297,860\n",
      "Non-trainable params: 10,689,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('improved_dog_disease_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
